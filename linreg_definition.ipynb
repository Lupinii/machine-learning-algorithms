{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca47ff5-b8f0-4db0-a69d-e30bc4ec455b",
   "metadata": {},
   "source": [
    "<h3>Linear Regression Algorithm</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f3510-562b-4f8a-b32a-e11410122e11",
   "metadata": {},
   "source": [
    "We are going to explain how linear regression estimates model coefficients using the least squares method.\n",
    "\n",
    "Linear regression models the relationship between a dependent variable (y) and one or more independent variables (X). In simple linear regression, we have one independent variable, and the model is:\n",
    "\n",
    "y = β0 + β1 * x + ε\n",
    "\n",
    "where:\n",
    "\n",
    "y is the dependent variable,\n",
    "\n",
    "x is the independent variable,\n",
    "\n",
    "β0 is the y-intercept,\n",
    "\n",
    "β1 is the slope of the line,\n",
    "\n",
    "ε is the error term.\n",
    "\n",
    "The goal is to find the values of β0 and β1 that minimize the sum of the squared residuals (errors). The residual for the i-th observation is the difference between the observed value (yi) and the predicted value (ŷi):\n",
    "\n",
    "ei = yi - ŷi = yi - (β0 + β1 * xi)\n",
    "\n",
    "The sum of squared residuals (SSR) is:\n",
    "\n",
    "SSR = Σ(ei)^2 = Σ[yi - (β0 + β1 * xi)]^2\n",
    "\n",
    "We minimize SSR with respect to β0 and β1.\n",
    "\n",
    "Derivation:\n",
    "\n",
    "To find the minimum, we take the partial derivatives of SSR with respect to β0 and β1 and set them to zero.\n",
    "\n",
    "1. Partial derivative with respect to β0:\n",
    "\n",
    "∂SSR/∂β0 = -2 * Σ[yi - β0 - β1 * xi] = 0\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "Σ(yi) - n*β0 - β1 * Σ(xi) = 0\n",
    "\n",
    "Rearranged:\n",
    "\n",
    "n*β0 + β1 * Σ(xi) = Σ(yi)        ... (Equation 1)\n",
    "\n",
    "2. Partial derivative with respect to β1:\n",
    "\n",
    "∂SSR/∂β1 = -2 * Σ[ (yi - β0 - β1 * xi) * xi ] = 0\n",
    "\n",
    "This simplifies to:\n",
    "\n",
    "Σ[xi * (yi - β0 - β1 * xi)] = 0\n",
    "\n",
    "Which becomes:\n",
    "\n",
    "Σ(xi*yi) - β0 * Σ(xi) - β1 * Σ(xi^2) = 0\n",
    "\n",
    "Rearranged:\n",
    "\n",
    "β0 * Σ(xi) + β1 * Σ(xi^2) = Σ(xi*yi)     ... (Equation 2)\n",
    "\n",
    "Now we have a system of two equations (Equation 1 and Equation 2) with two unknowns (β0 and β1). This system is often called the \"normal equations\".\n",
    "\n",
    "Solving the system:\n",
    "\n",
    "From Equation 1:\n",
    "\n",
    "β0 = [Σ(yi) - β1 * Σ(xi)] / n\n",
    "\n",
    "Substitute β0 into Equation 2:\n",
    "\n",
    "[ (Σ(yi) - β1 * Σ(xi)) / n ] * Σ(xi) + β1 * Σ(xi^2) = Σ(xi*yi)\n",
    "\n",
    "Multiply both sides by n:\n",
    "\n",
    "(Σ(yi) - β1 * Σ(xi)) * Σ(xi) + β1 * Σ(xi^2) * n = n * Σ(xi*yi)\n",
    "\n",
    "Rearrange:\n",
    "\n",
    "Σ(yi)*Σ(xi) - β1 * [Σ(xi)]^2 + β1 * n * Σ(xi^2) = n * Σ(xi*yi)\n",
    "\n",
    "Bring terms with β1 to one side:\n",
    "\n",
    "β1 * [ n * Σ(xi^2) - (Σ(xi))^2 ] = n * Σ(xi*yi) - Σ(yi)*Σ(xi)\n",
    "\n",
    "Therefore:\n",
    "\n",
    "β1 = [ n * Σ(xi*yi) - Σ(xi) * Σ(yi) ] / [ n * Σ(xi^2) - (Σ(xi))^2 ]\n",
    "\n",
    "We can also write:\n",
    "\n",
    "β1 = [ Σ(xi*yi) - (Σ(xi)*Σ(yi))/n ] / [ Σ(xi^2) - (Σ(xi))^2 / n ]\n",
    "\n",
    "Which is equivalent to:\n",
    "\n",
    "β1 = Covariance(x, y) / Variance(x)\n",
    "\n",
    "because:\n",
    "\n",
    "Covariance(x, y) = [ Σ(xi*yi) - (Σ(xi)*Σ(yi))/n ] / (n-1)   [but note we are using n without adjustment in the denominator for population variance? Actually, in the formula above we have a denominator without n-1, so we are using the formula for the population variance and covariance? But note that the n cancels in the formula for the slope when we use the sample formulas. However, the formula we have is the standard one for simple linear regression.]\n",
    "\n",
    "Alternatively, we can write:\n",
    "\n",
    "β1 = Σ[ (xi - x̄)(yi - ȳ) ] / Σ[ (xi - x̄)^2 ]\n",
    "\n",
    "where x̄ and ȳ are the sample means.\n",
    "\n",
    "Then, once we have β1, we can compute β0:\n",
    "\n",
    "β0 = ȳ - β1 * x̄\n",
    "\n",
    "This is the same as from Equation 1: β0 = [Σ(yi) - β1 * Σ(xi)] / n = ȳ - β1 * x̄.\n",
    "\n",
    "So, the formulas are:\n",
    "\n",
    "β1 = [ n * Σ(xi*yi) - Σ(xi)*Σ(yi) ] / [ n * Σ(xi^2) - (Σ(xi))^2 ]\n",
    "\n",
    "β0 = ȳ - β1 * x̄\n",
    "\n",
    "Visual Example:\n",
    "\n",
    "Let's create a simple example with 3 data points:\n",
    "\n",
    "Points: (1, 2), (2, 3), (3, 5)\n",
    "\n",
    "We want to fit a line: y = β0 + β1 * x\n",
    "\n",
    "Step 1: Calculate the necessary sums.\n",
    "\n",
    "n = 3\n",
    "\n",
    "Σxi = 1+2+3 = 6\n",
    "\n",
    "Σyi = 2+3+5 = 10\n",
    "\n",
    "Σ(xi*yi) = 1*2 + 2*3 + 3*5 = 2 + 6 + 15 = 23\n",
    "\n",
    "Σ(xi^2) = 1^2 + 2^2 + 3^2 = 1+4+9 = 14\n",
    "\n",
    "Step 2: Calculate β1.\n",
    "\n",
    "numerator = n * Σ(xi*yi) - Σxi * Σyi = 3*23 - 6*10 = 69 - 60 = 9\n",
    "\n",
    "denominator = n * Σ(xi^2) - (Σxi)^2 = 3*14 - 6^2 = 42 - 36 = 6\n",
    "\n",
    "β1 = 9/6 = 1.5\n",
    "\n",
    "Step 3: Calculate β0.\n",
    "\n",
    "x̄ = Σxi / n = 6/3 = 2\n",
    "\n",
    "ȳ = Σyi / n = 10/3 ≈ 3.333\n",
    "\n",
    "β0 = ȳ - β1 * x̄ = 3.333 - 1.5 * 2 = 3.333 - 3 = 0.333\n",
    "\n",
    "So the regression line is: y = 0.333 + 1.5 * x\n",
    "\n",
    "Now, let's visualize:\n",
    "\n",
    "We have points: (1,2), (2,3), (3,5)\n",
    "\n",
    "The line:\n",
    "\n",
    "when x=1: y=0.333+1.5=1.833\n",
    "\n",
    "when x=2: y=0.333+3=3.333\n",
    "\n",
    "when x=3: y=0.333+4.5=4.833\n",
    "\n",
    "We can plot these points and the line.\n",
    "\n",
    "But note: the observed y at x=1 is 2, predicted is 1.833 -> residual = 2-1.833=0.167\n",
    "\n",
    "at x=2: observed=3, predicted=3.333 -> residual=-0.333\n",
    "\n",
    "at x=3: observed=5, predicted=4.833 -> residual=0.167\n",
    "\n",
    "The sum of squared residuals = (0.167)^2 + (-0.333)^2 + (0.167)^2 ≈ 0.0279 + 0.1109 + 0.0279 = 0.1667\n",
    "\n",
    "We can check that this is the minimum by trying a small change in the slope or intercept and seeing that the SSR increases.\n",
    "\n",
    "This is how linear regression uses the least squares method to estimate the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bfb09-d5fa-4124-a18f-9e5c5772707a",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>Impact of Multicollinearity and Outliers on Linear Regression</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cec722-c3e8-4b89-ae12-70aab8328308",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>1. Multicollinearity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51841a3b-5887-4de2-94b1-416708753d86",
   "metadata": {},
   "source": [
    "Definition: When two or more independent variables are highly correlated, making it difficult to isolate their individual effects on the dependent variable.\n",
    "\n",
    "Effects:\n",
    "\n",
    "Unstable coefficient estimates: Small changes in data can cause large changes \n",
    "\n",
    "Inflated standard errors: Reduces statistical significance (high p-values).\n",
    "\n",
    "Misleading interpretation: Coefficients may have wrong signs or magnitudes.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Correlation Matrix: Check for \n",
    "∣r∣>0.8 between predictors.\n",
    "\n",
    "Variance Inflation Factor (VIF):\n",
    "\n",
    "VIF > 5: Moderate multicollinearity.\n",
    "\n",
    "VIF > 10: Severe multicollinearity.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Remove redundant variables (e.g., keep one from a pair of highly correlated predictors).\n",
    "\n",
    "Use regularization (Ridge Regression/Lasso) to penalize large coefficients.\n",
    "\n",
    "Principal Component Analysis (PCA) to transform predictors into uncorrelated components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf1149-9479-47f1-b986-f977ed0bf64e",
   "metadata": {},
   "source": [
    "x2\n",
    " |     /\n",
    " |    /   (x1 and x2 are highly correlated)\n",
    " |   /\n",
    " |  /\n",
    " +---------- x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ed256-cc12-41b5-bc84-d53b2f31f36e",
   "metadata": {},
   "source": [
    "If x1≈x2, the model cannot distinguish their effects on y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d9687-ac6a-4a7f-b76d-a6e725b69d13",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>2. Outliers</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a6541-ea02-4a9f-ac5b-e06c750e9d02",
   "metadata": {},
   "source": [
    "Definition: Data points that deviate significantly from the trend.\n",
    "\n",
    "Effects:\n",
    "\n",
    "Bias coefficient estimates: Outliers can \"pull\" the regression line.\n",
    "\n",
    "Increase residuals: Hurt model accuracy (e.g., high MSE).\n",
    "\n",
    "Skew statistical tests: Affect p-values and confidence intervals.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Leverage (Hat Values): Measures influence of \n",
    "Studentized Residuals:\n",
    " ∣>3 suggests an outlier.\n",
    "\n",
    "Cook’s Distance: Combines leverage and residuals.\n",
    " >1 indicates high influence.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "Remove outliers if they are data errors.\n",
    "\n",
    "Transform variables (e.g., log transform to reduce skew).\n",
    "\n",
    "Use robust regression (e.g., RANSAC, Huber Regressor) that downweights outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b2b5e6-7ef7-4cd4-8c55-43663fc6a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y\n",
    " |        * (outlier)\n",
    " |       /\n",
    " |      * \n",
    " |     / \n",
    " | * * * * \n",
    " +---------- x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b9dbd-dbca-4d99-accf-c59e3c036754",
   "metadata": {},
   "source": [
    "The outlier drags the regression line away from the true trend.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b4130-45a4-4b67-be74-0ce770aca51c",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>Practical Workflow</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edc7fd-cccb-429d-add2-d5596444bacd",
   "metadata": {},
   "source": [
    "Check for Multicollinearity:\n",
    "\n",
    "Compute VIFs.\n",
    "\n",
    "Drop/replace collinear variables.\n",
    "\n",
    "Detect Outliers:\n",
    "\n",
    "Plot residuals vs. fitted values.\n",
    "\n",
    "Calculate Cook’s distance.\n",
    "\n",
    "Mitigate Issues:\n",
    "\n",
    "For multicollinearity: Ridge/Lasso regression.\n",
    "\n",
    "For outliers: Robust regression or winsorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286cb807-3183-40b0-8559-9c38f1c953cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity check\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# Outlier detection\n",
    "from statsmodels.stats.diagnostic import influence_plot\n",
    "influence_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62441cbd-1d2d-4551-97ee-638933f99bd4",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>Key Takeaways</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02601dba-222c-4da9-bcb8-12f93c3c318f",
   "metadata": {},
   "source": [
    "Multicollinearity destabilizes coefficients; use VIF and regularization.\n",
    "\n",
    "Outliers distort predictions; detect them with residuals/Cook’s distance and use robust methods.\n",
    "\n",
    "Always visualize data (e.g., scatter plots, residual plots) alongside statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fcf7e-cbc6-4f60-a328-0cfbd6e06a83",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>Comparing simple linear regression with multiple linear regression using a real-world dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8929f-4b62-44c2-bd3d-575d331696f1",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "1. Load the dataset (using sklearn's version).\n",
    "\n",
    "2. Perform simple linear regression using one feature (e.g., 'RM' - average number of rooms).\n",
    "\n",
    "3. Perform multiple linear regression using multiple features (e.g., 'RM', 'LSTAT' - lower status of the population, 'CRIM' - per capita crime rate).\n",
    "\n",
    "4. Compare the models and highlight interpretation differences.\n",
    "\n",
    "Interpretation differences:\n",
    "\n",
    "- In simple linear regression: We interpret the coefficient as the change in the target for a one-unit change in the predictor, holding nothing else constant (since there are no other predictors).\n",
    "\n",
    "- In multiple linear regression: We interpret each coefficient as the change in the target for a one-unit change in the predictor, holding all other predictors constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee18b6-7326-4f82-bc48-035047be76da",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>Real-World Example: Boston Housing Dataset</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9889f82-c978-488e-879f-70b28e661d56",
   "metadata": {},
   "source": [
    "Goal: Predict median home value (MEDV) using:\n",
    "\n",
    "SLR: Only RM (average rooms per dwelling)\n",
    "\n",
    "MLR: RM + LSTAT (% lower-status population) + CRIM (per-capita crime rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886c3d6-e285-4bfa-9524-79b023a3a318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load diabetes dataset (using diabetes for demonstration; Boston deprecated)\n",
    "data = load_diabetes()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target (disease progression)\n",
    "\n",
    "# SLR: Use only one feature (BMI)\n",
    "slr = LinearRegression()\n",
    "slr.fit(X[:, np.newaxis, 2], y)  # Using BMI (feature index 2)\n",
    "slr_r2 = r2_score(y, slr.predict(X[:, np.newaxis, 2]))\n",
    "\n",
    "# MLR: Use BMI + BP + S3 (3 features)\n",
    "mlr = LinearRegression()\n",
    "mlr.fit(X[:, [2, 3, 6]], y)  # Features: BMI, BP, S3\n",
    "mlr_r2 = r2_score(y, mlr.predict(X[:, [2, 3, 6]]))\n",
    "\n",
    "print(f\"SLR R²: {slr_r2:.3f} | MLR R²: {mlr_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4ee9c7-6583-48ac-af3f-003ca0b02b94",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>Output:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf663da-fb2e-43a7-891a-655e5a50ee7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLR R²: 0.343 | MLR R²: 0.416"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e24f6-2253-40f5-b333-255b1e11c140",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>Visual Comparison</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9faa96-3df7-4b65-a0b5-73a46821e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# SLR plot\n",
    "sns.scatterplot(x=y, y=slr.predict(X[:, np.newaxis, 2]), ax=ax1)\n",
    "ax1.set_title(f'SLR (R²={slr_r2:.2f})')\n",
    "ax1.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "\n",
    "# MLR plot\n",
    "sns.scatterplot(x=y, y=mlr.predict(X[:, [2, 3, 6]]), ax=ax2)\n",
    "ax2.set_title(f'MLR (R²={mlr_r2:.2f})')\n",
    "ax2.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
