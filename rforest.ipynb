{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66750e78-c015-48f6-a410-39b56fdf6896",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Random Forest Algorithm</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50a4b3-4c8c-4566-954a-1e14f4cf0f0b",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>How the Random Forest Algorithm Works</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4162f-725d-4437-b4d8-76234c5dfa68",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning method that constructs multiple decision trees during training and combines their predictions to improve accuracy and reduce overfitting. Here's a step-by-step breakdown of how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed433b0-4d87-460d-8521-9b27e9262c09",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>1. Bootstrapping (Creating Multiple Datasets)</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb8eb5-756d-44b4-b52a-68f186a6ed09",
   "metadata": {},
   "source": [
    "Random Forest uses bagging (Bootstrap Aggregating) to create diverse subsets of the original dataset.\n",
    "\n",
    "For each decision tree:\n",
    "\n",
    "A random sample of the training data is selected with replacement (meaning some rows may be repeated).\n",
    "\n",
    "This results in different subsets of data for each tree, introducing variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da729d-aa27-4576-a17d-a420ffe74d30",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>2. Building Decision Trees with Random Feature Selection</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35912e6a-11a0-4030-888f-f0cf5e95e8ae",
   "metadata": {},
   "source": [
    "Each decision tree is trained on its bootstrapped dataset.\n",
    "\n",
    "At each split node in the tree:\n",
    "\n",
    "Instead of considering all features, only a random subset of features is evaluated (typically sqrt(n_features) for classification or n_features/3 for regression).\n",
    "\n",
    "The best feature from this subset is chosen to split the node.\n",
    "\n",
    "This randomness ensures trees are decorrelated, reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce8388-54fd-4ed4-9d14-ccc9dae9fdb4",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>3. Growing Trees to Maximum Depth (No Pruning)</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71866214-0e9a-49e7-9f8b-03cf4111b125",
   "metadata": {},
   "source": [
    "Trees are grown fully (or until a specified depth) without pruning.\n",
    "\n",
    "This means individual trees may overfit, but the ensemble averages out errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db196bbf-4ac3-45ec-a12f-7fd69e8b549f",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>4. Combining Predictions (Aggregation)</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a59354-47fa-4835-8576-eddc661bd5bd",
   "metadata": {},
   "source": [
    "For classification:\n",
    "\n",
    "Each tree votes for a class, and the majority vote determines the final prediction.\n",
    "\n",
    "For regression:\n",
    "\n",
    "The average of all tree predictions is taken as the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b58de-cadb-4325-b87d-ce16b9f48650",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Why Random Forest Works Well</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61572f5-b7e5-47eb-ae8e-557a42161141",
   "metadata": {},
   "source": [
    "Reduces Overfitting:\n",
    "\n",
    "By averaging multiple trees, it cancels out individual biases.\n",
    "\n",
    "Handles Noisy Data:\n",
    "\n",
    "Outliers affect only some trees, not the whole forest.\n",
    "\n",
    "Works with High-Dimensional Data:\n",
    "\n",
    "Feature randomness ensures different trees learn different patterns.\n",
    "\n",
    "Requires Little Preprocessing:\n",
    "\n",
    "No need for feature scaling; handles missing values well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fda34a-b944-4698-b8b9-81663010851f",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Example: Classification with Random Forest</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9da7b7-ba13-416a-8aea-a20908db9995",
   "metadata": {},
   "source": [
    "Suppose we have a dataset to predict whether a person likes movies based on age and gender.\n",
    "\n",
    "Bootstrapping:\n",
    "\n",
    "Create 3 decision trees, each trained on a random subset of data.\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "At each split, only one random feature (age or gender) is considered.\n",
    "\n",
    "Prediction:\n",
    "\n",
    "Tree 1 predicts Yes, Tree 2 predicts No, Tree 3 predicts Yes.\n",
    "\n",
    "Final prediction = Yes (majority vote)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056fb56-6f11-4a1f-9491-c83f0272dc8f",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Key Hyperparameters</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c980ece-61be-40dd-bbe0-c4ea4ba2aeea",
   "metadata": {},
   "source": [
    "n_estimators: Number of trees (more trees = better accuracy, but slower).\n",
    "\n",
    "max_features: Number of features considered at each split.\n",
    "\n",
    "max_depth: Maximum depth of each tree.\n",
    "\n",
    "min_samples_split: Minimum samples required to split a node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9c614-90fe-445d-ac22-ab7017e411aa",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Advantages of Random Forest</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c6399-27c6-47d8-956a-41f9f80d7439",
   "metadata": {},
   "source": [
    "✅ High accuracy (better than single decision trees).\n",
    "✅ Robust to noise and outliers.\n",
    "✅ Handles both classification and regression.\n",
    "✅ Provides feature importance scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55d334-a58a-4fa5-a30c-255151f0a35b",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Disadvantages</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527ce70-9e34-4a64-a5e8-83dc3f3cba1d",
   "metadata": {},
   "source": [
    "❌ Slower prediction than single trees (but parallelizable).\n",
    "❌ Less interpretable than a single decision tre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd71278-8716-4371-ac2e-314e2500a8f4",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Conclusion</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6d6ab-3867-4f47-b74b-a1c9b5d5e574",
   "metadata": {},
   "source": [
    "Random Forest improves model performance by combining multiple decorrelated decision trees through bagging and random feature selection, then aggregating their predictions. This ensemble approach reduces variance and overfitting while maintaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb734e-9d05-40c0-a281-d8b2049d4ee0",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>complete example of training a Random Forest classifier using scikit-learn, including feature importance and model evaluation.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32064e8-7416-4af3-ba28-84b73ae20107",
   "metadata": {},
   "source": [
    "We are going to:\n",
    "\n",
    "1. Import necessary libraries.\n",
    "\n",
    "2. Load a dataset (we'll use the Iris dataset for classification).\n",
    "\n",
    "3. Split the data into training and test sets.\n",
    "\n",
    "4. Train a Random Forest classifier.\n",
    "\n",
    "5. Make predictions on the test set.\n",
    "\n",
    "6. Evaluate the model (accuracy, confusion matrix, classification report).\n",
    "\n",
    "7. Display feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d55a8-14b4-4911-823d-63a72c538a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load and prepare data\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (sepal/petal measurements)\n",
    "y = iris.target  # Target (flower species)\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Split data: 70% training, 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Initialize and train Random Forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    max_depth=3,       # Restrict tree depth\n",
    "    max_features='sqrt',# Features considered per split: sqrt(4)=2\n",
    "    random_state=42,\n",
    "    n_jobs=-1          # Use all CPU cores\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# 4. Evaluate model\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred).round(2))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 5. Feature Importance Analysis\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# Sort feature importances\n",
    "sorted_idx = importances.argsort()[::-1]\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Feature Importances\")\n",
    "sns.barplot(x=importances[sorted_idx], y=np.array(feature_names)[sorted_idx], \n",
    "            xerr=std[sorted_idx], palette=\"viridis\")\n",
    "plt.xlabel(\"Importance Score (Gini Importance)\")\n",
    "plt.show()\n",
    "\n",
    "# Print numerical importance values\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, score in zip(np.array(feature_names)[sorted_idx], importances[sorted_idx]):\n",
    "    print(f\"{name}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d19872-0a80-4537-8cd8-21c0e15b418d",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Data Preparation:</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b281d3-11f8-4680-9891-9a24aaa9a683",
   "metadata": {},
   "source": [
    "Uses the Iris dataset (150 samples, 4 features, 3 classes)\n",
    "\n",
    "70-30 train-test split with stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41075404-0570-4b34-ac57-aebaac38bf95",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Model Configuration:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64268d03-7679-4b11-9f62-05ed6d2058be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators=100: Builds 100 decision trees\n",
    "\n",
    "max_depth=3: Controls tree complexity to prevent overfitting\n",
    "\n",
    "max_features='sqrt': Randomly selects √4 = 2 features per split\n",
    "\n",
    "random_state=42: Ensures reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9342150-ca25-4c56-aa98-6565fd38f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h5 style='color:black;'>Evaluation Metrics:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f75d8-84c2-41ad-990e-6ed0520a3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: Overall correct prediction rate\n",
    "\n",
    "Confusion Matrix: Visualizes true vs predicted classes\n",
    "\n",
    "Classification Report: Precision, recall, F1-score per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018378f-bdba-4294-8a0a-4a1fb1302374",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h5 style='color:black;'>Feature Importance:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b858839a-8bd7-4b9f-afba-a5191b74a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "Measures mean decrease in Gini impurity caused by each feature\n",
    "\n",
    "Visualizes importance scores with standard deviation across trees\n",
    "\n",
    "Petal dimensions typically show highest importance in Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32de6f-6450-4eb4-9a4b-80f9cd54d9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h5 style='color:black;'>Classification Report:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bcdac-e035-4a2a-8321-630e3ef65df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "      setosa       1.00      1.00      1.00        19\n",
    "  versicolor       1.00      0.92      0.96        13\n",
    "   virginica       0.93      1.00      0.96        13\n",
    "\n",
    "    accuracy                           0.98        45\n",
    "   macro avg       0.98      0.97      0.97        45\n",
    "weighted avg       0.98      0.98      0.98        45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9c40c-6a97-4bc1-bd0d-7048d5a81279",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h5 style='color:black;'>Numerical Importances:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bfb40-b156-4a68-9345-a68340de2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "petal length (cm): 0.467\n",
    "petal width (cm): 0.424\n",
    "sepal length (cm): 0.088\n",
    "sepal width (cm): 0.021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b68bb0-4ccc-44d9-9af2-2004ee0617cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "<h5 style='color:black;'>Key Insights:</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8666e1f-7b18-48b3-9eb9-d4e4a483cc5d",
   "metadata": {},
   "source": [
    "High Accuracy: Typically achieves 95-100% on Iris with proper parameters\n",
    "\n",
    "Feature Insights:\n",
    "\n",
    "Petal measurements dominate importance (>85% combined)\n",
    "\n",
    "Sepal width has negligible predictive power\n",
    "\n",
    "Error Analysis:\n",
    "\n",
    "Most confusion occurs between versicolor/virginica\n",
    "\n",
    "Setosa is perfectly separable\n",
    "\n",
    "This example demonstrates the end-to-end process of training a Random Forest classifier while highlighting its key advantages: automatic feature selection, robustness to overfitting, and interpretable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
