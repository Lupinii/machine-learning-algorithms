{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66750e78-c015-48f6-a410-39b56fdf6896",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Random Forest Algorithm</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50a4b3-4c8c-4566-954a-1e14f4cf0f0b",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>How the Random Forest Algorithm Works</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4162f-725d-4437-b4d8-76234c5dfa68",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning method that constructs multiple decision trees during training and combines their predictions to improve accuracy and reduce overfitting. Here's a step-by-step breakdown of how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed433b0-4d87-460d-8521-9b27e9262c09",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>1. Bootstrapping (Creating Multiple Datasets)</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cb8eb5-756d-44b4-b52a-68f186a6ed09",
   "metadata": {},
   "source": [
    "Random Forest uses bagging (Bootstrap Aggregating) to create diverse subsets of the original dataset.\n",
    "\n",
    "For each decision tree:\n",
    "\n",
    "A random sample of the training data is selected with replacement (meaning some rows may be repeated).\n",
    "\n",
    "This results in different subsets of data for each tree, introducing variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77da729d-aa27-4576-a17d-a420ffe74d30",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>2. Building Decision Trees with Random Feature Selection</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35912e6a-11a0-4030-888f-f0cf5e95e8ae",
   "metadata": {},
   "source": [
    "Each decision tree is trained on its bootstrapped dataset.\n",
    "\n",
    "At each split node in the tree:\n",
    "\n",
    "Instead of considering all features, only a random subset of features is evaluated (typically sqrt(n_features) for classification or n_features/3 for regression).\n",
    "\n",
    "The best feature from this subset is chosen to split the node.\n",
    "\n",
    "This randomness ensures trees are decorrelated, reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce8388-54fd-4ed4-9d14-ccc9dae9fdb4",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>3. Growing Trees to Maximum Depth (No Pruning)</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71866214-0e9a-49e7-9f8b-03cf4111b125",
   "metadata": {},
   "source": [
    "Trees are grown fully (or until a specified depth) without pruning.\n",
    "\n",
    "This means individual trees may overfit, but the ensemble averages out errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db196bbf-4ac3-45ec-a12f-7fd69e8b549f",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>4. Combining Predictions (Aggregation)</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a59354-47fa-4835-8576-eddc661bd5bd",
   "metadata": {},
   "source": [
    "For classification:\n",
    "\n",
    "Each tree votes for a class, and the majority vote determines the final prediction.\n",
    "\n",
    "For regression:\n",
    "\n",
    "The average of all tree predictions is taken as the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b58de-cadb-4325-b87d-ce16b9f48650",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Why Random Forest Works Well</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61572f5-b7e5-47eb-ae8e-557a42161141",
   "metadata": {},
   "source": [
    "Reduces Overfitting:\n",
    "\n",
    "By averaging multiple trees, it cancels out individual biases.\n",
    "\n",
    "Handles Noisy Data:\n",
    "\n",
    "Outliers affect only some trees, not the whole forest.\n",
    "\n",
    "Works with High-Dimensional Data:\n",
    "\n",
    "Feature randomness ensures different trees learn different patterns.\n",
    "\n",
    "Requires Little Preprocessing:\n",
    "\n",
    "No need for feature scaling; handles missing values well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fda34a-b944-4698-b8b9-81663010851f",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Example: Classification with Random Forest</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9da7b7-ba13-416a-8aea-a20908db9995",
   "metadata": {},
   "source": [
    "Suppose we have a dataset to predict whether a person likes movies based on age and gender.\n",
    "\n",
    "Bootstrapping:\n",
    "\n",
    "Create 3 decision trees, each trained on a random subset of data.\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "At each split, only one random feature (age or gender) is considered.\n",
    "\n",
    "Prediction:\n",
    "\n",
    "Tree 1 predicts Yes, Tree 2 predicts No, Tree 3 predicts Yes.\n",
    "\n",
    "Final prediction = Yes (majority vote)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056fb56-6f11-4a1f-9491-c83f0272dc8f",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Key Hyperparameters</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c980ece-61be-40dd-bbe0-c4ea4ba2aeea",
   "metadata": {},
   "source": [
    "n_estimators: Number of trees (more trees = better accuracy, but slower).\n",
    "\n",
    "max_features: Number of features considered at each split.\n",
    "\n",
    "max_depth: Maximum depth of each tree.\n",
    "\n",
    "min_samples_split: Minimum samples required to split a node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9c614-90fe-445d-ac22-ab7017e411aa",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Advantages of Random Forest</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2c6399-27c6-47d8-956a-41f9f80d7439",
   "metadata": {},
   "source": [
    "✅ High accuracy (better than single decision trees).\n",
    "✅ Robust to noise and outliers.\n",
    "✅ Handles both classification and regression.\n",
    "✅ Provides feature importance scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55d334-a58a-4fa5-a30c-255151f0a35b",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Disadvantages</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2527ce70-9e34-4a64-a5e8-83dc3f3cba1d",
   "metadata": {},
   "source": [
    "❌ Slower prediction than single trees (but parallelizable).\n",
    "❌ Less interpretable than a single decision tre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd71278-8716-4371-ac2e-314e2500a8f4",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Conclusion</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c6d6ab-3867-4f47-b74b-a1c9b5d5e574",
   "metadata": {},
   "source": [
    "Random Forest improves model performance by combining multiple decorrelated decision trees through bagging and random feature selection, then aggregating their predictions. This ensemble approach reduces variance and overfitting while maintaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fb734e-9d05-40c0-a281-d8b2049d4ee0",
   "metadata": {},
   "source": [
    "<h3 style='color:black;'>complete example of training a Random Forest classifier using scikit-learn, including feature importance and model evaluation.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32064e8-7416-4af3-ba28-84b73ae20107",
   "metadata": {},
   "source": [
    "We are going to:\n",
    "\n",
    "1. Import necessary libraries.\n",
    "\n",
    "2. Load a dataset (we'll use the Iris dataset for classification).\n",
    "\n",
    "3. Split the data into training and test sets.\n",
    "\n",
    "4. Train a Random Forest classifier.\n",
    "\n",
    "5. Make predictions on the test set.\n",
    "\n",
    "6. Evaluate the model (accuracy, confusion matrix, classification report).\n",
    "\n",
    "7. Display feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d55a8-14b4-4911-823d-63a72c538a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load and prepare data\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (sepal/petal measurements)\n",
    "y = iris.target  # Target (flower species)\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Split data: 70% training, 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Initialize and train Random Forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees\n",
    "    max_depth=3,       # Restrict tree depth\n",
    "    max_features='sqrt',# Features considered per split: sqrt(4)=2\n",
    "    random_state=42,\n",
    "    n_jobs=-1          # Use all CPU cores\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 3. Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# 4. Evaluate model\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred).round(2))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 5. Feature Importance Analysis\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# Sort feature importances\n",
    "sorted_idx = importances.argsort()[::-1]\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Feature Importances\")\n",
    "sns.barplot(x=importances[sorted_idx], y=np.array(feature_names)[sorted_idx], \n",
    "            xerr=std[sorted_idx], palette=\"viridis\")\n",
    "plt.xlabel(\"Importance Score (Gini Importance)\")\n",
    "plt.show()\n",
    "\n",
    "# Print numerical importance values\n",
    "print(\"\\nFeature Importances:\")\n",
    "for name, score in zip(np.array(feature_names)[sorted_idx], importances[sorted_idx]):\n",
    "    print(f\"{name}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d19872-0a80-4537-8cd8-21c0e15b418d",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Data Preparation:</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b281d3-11f8-4680-9891-9a24aaa9a683",
   "metadata": {},
   "source": [
    "Uses the Iris dataset (150 samples, 4 features, 3 classes)\n",
    "\n",
    "70-30 train-test split with stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41075404-0570-4b34-ac57-aebaac38bf95",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Model Configuration:</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d5b3a2-9564-4d33-801f-30bcc586afdf",
   "metadata": {},
   "source": [
    "n_estimators=100: Builds 100 decision trees\n",
    "\n",
    "max_depth=3: Controls tree complexity to prevent overfitting\n",
    "\n",
    "max_features='sqrt': Randomly selects √4 = 2 features per split\n",
    "\n",
    "random_state=42: Ensures reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8624f5-00b4-45f3-b4a2-ffe242e7ee1e",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Evaluation Metrics:</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e158dd5-69a6-4ce3-95f5-d3aba7043ce5",
   "metadata": {},
   "source": [
    "Accuracy: Overall correct prediction rate\n",
    "\n",
    "Confusion Matrix: Visualizes true vs predicted classes\n",
    "\n",
    "Classification Report: Precision, recall, F1-score per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eebadf-10d5-43c3-b4f3-17c66c3aadee",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Feature Importance:</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bdb06e-acfe-443e-887f-4c6029e78a89",
   "metadata": {},
   "source": [
    "Measures mean decrease in Gini impurity caused by each feature\n",
    "\n",
    "Visualizes importance scores with standard deviation across trees\n",
    "\n",
    "Petal dimensions typically show highest importance in Iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752dd0c-b426-4a24-b86d-8dc629929988",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Classification Report:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bcdac-e035-4a2a-8321-630e3ef65df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "      setosa       1.00      1.00      1.00        19\n",
    "  versicolor       1.00      0.92      0.96        13\n",
    "   virginica       0.93      1.00      0.96        13\n",
    "\n",
    "    accuracy                           0.98        45\n",
    "   macro avg       0.98      0.97      0.97        45\n",
    "weighted avg       0.98      0.98      0.98        45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58215304-ad70-407d-bf27-d54017224eac",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Numerical Importances:</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47bfb40-b156-4a68-9345-a68340de2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "petal length (cm): 0.467\n",
    "petal width (cm): 0.424\n",
    "sepal length (cm): 0.088\n",
    "sepal width (cm): 0.021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa94d8f4-8828-4a74-bece-65ed0701569c",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Key Insights:</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8666e1f-7b18-48b3-9eb9-d4e4a483cc5d",
   "metadata": {},
   "source": [
    "High Accuracy: Typically achieves 95-100% on Iris with proper parameters\n",
    "\n",
    "Feature Insights:\n",
    "\n",
    "Petal measurements dominate importance (>85% combined)\n",
    "\n",
    "Sepal width has negligible predictive power\n",
    "\n",
    "Error Analysis:\n",
    "\n",
    "Most confusion occurs between versicolor/virginica\n",
    "\n",
    "Setosa is perfectly separable\n",
    "\n",
    "This example demonstrates the end-to-end process of training a Random Forest classifier while highlighting its key advantages: automatic feature selection, robustness to overfitting, and interpretable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f572a6-add7-4871-a808-11b0dfffc9b0",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>How hyperparameters like n_estimators, max_depth, and max_features affect the performance of a Random Forest model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a81c52-690b-4c0e-9b71-9e197c3b4eda",
   "metadata": {},
   "source": [
    "We are going to discuss the effects of three key hyperparameters in Random Forest:\n",
    "\n",
    "1. n_estimators: number of trees in the forest.\n",
    "\n",
    "2. max_depth: maximum depth of each tree.\n",
    "\n",
    "3. max_features: number of features to consider when looking for the best split at each node.\n",
    "\n",
    "We'll break down each hyperparameter and explain how they impact:\n",
    "\n",
    "- Model performance (accuracy, generalization)\n",
    "\n",
    "- Training time\n",
    "\n",
    "- Overfitting vs. Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3546911e-0833-4716-a19a-069045d718e5",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Impact of Random Forest Hyperparameters on Model Performance</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5e4d83-a15e-42b9-82f2-4c718a525812",
   "metadata": {},
   "source": [
    "Hyperparameters control the trade-off between bias-variance, training time, and model complexity. Here's how n_estimators, max_depth, and max_features affect performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1eacc-c5bc-4ee4-88e5-1022944e208c",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>1. n_estimators (Number of Trees)</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca562bc7-f4e7-4faf-8259-c11c88f194f5",
   "metadata": {},
   "source": [
    "Value\t         Effect on Model\t                                                          Risk\n",
    "Too Low\t      High variance, unstable predictions (underfitting)\t                      ❌ Poor generalization\n",
    "Increasing\t  ↓ Variance, ↑ accuracy (more trees average out errors)\t                  ✅ Improves robustness\n",
    "Too High\t  Diminishing returns; slows training/prediction without meaningful gains\t  ❌ Computationally expens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c2128-8a8b-49e0-98d3-82ae472297f2",
   "metadata": {},
   "source": [
    "Optimal Setting:\n",
    "\n",
    "Start with 100-200 trees, increase until validation score plateaus.\n",
    "\n",
    "Tip: Use early stopping (warm_start=True) to find optimal number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146f6ec-613e-4e26-b5d1-2dae2f6b0ccd",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>2. max_depth (Tree Depth)</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cba045-b7f9-4ff0-aea9-fc46413c37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Value\t         Effect on Model\t                                       Risk\n",
    "Shallow\t     High bias, simple trees (underfitting)\t                ❌ Fails to capture complex patterns\n",
    "Increasing\t ↓ Bias, ↑ granularity (trees learn detailed rules)\t    ✅ Better accuracy on training data\n",
    "Too Deep\t High variance, overfitting; trees memorize noise\t    ❌ Poor generalization on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367011f5-e21e-48fa-a4ed-7738c2fc5c97",
   "metadata": {},
   "source": [
    "Optimal Setting:\n",
    "\n",
    "Start with max_depth=None (unlimited), then restrict if overfitting occurs.\n",
    "\n",
    "Typical range: 5-30 (validate with learning curves)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38302bed-fcfb-4f17-9fb3-a9699da0b143",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>3. max_features (Features per Split)</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c17a4-7ebf-4216-8da4-5e58a8e5ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Value\t             Effect on Model\t                                         Risk\n",
    "Low (e.g., 1)\t High tree diversity, ↑ decorrelation\t                 ✅ Reduces overfitting\n",
    "Increasing\t     Trees become more similar, ↓ diversity\t                 ❌ ↑ Risk of overfitting\n",
    "Too High\t     Trees behave identically (defeats ensemble purpose)\t ❌ ↑ Variance, ↓ robustness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b0087-1a5b-4ff4-a2eb-3a455720cf53",
   "metadata": {},
   "source": [
    "Optimal Setting:\n",
    "\n",
    "Classification: sqrt(n_features)\n",
    "\n",
    "Regression: n_features / 3\n",
    "\n",
    "Tuning range: 0.1–1.0 (ratio) or 1–n_features (integer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c8d8d3-3a6c-4a6e-8796-1116ddeb954e",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Interaction Effects & Best Practices</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b77336-8927-4f7c-b87b-c2fceefb6e5c",
   "metadata": {},
   "source": [
    "Bias-Variance Tradeoff:\n",
    "\n",
    "↑ max_depth + ↑ n_estimators → ↑ Risk of overfitting\n",
    "\n",
    "↓ max_depth + ↑ max_features → ↑ Risk of underfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf8485-066c-4ebd-9c19-a32d8bda95a9",
   "metadata": {},
   "source": [
    "Validation Strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bd9c2-1938-4955-aa82-537ca7428518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'max_features': ['sqrt', 'log2', 0.8]\n",
    "}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2fc4b-d8a7-4aa0-b02b-32126e638579",
   "metadata": {},
   "outputs": [],
   "source": [
    "Performance Guidelines:\n",
    "Symptom\tSolution\n",
    "High training accuracy, low test accuracy\t↓ max_depth, ↑ max_features, ↓ n_estimators\n",
    "Low training accuracy\t↑ max_depth, ↓ max_features, ↑ n_estimators\n",
    "Slow training\t↓ n_estimators, ↓ max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4e0e8-c5d4-4ff3-988c-52f92cf3b34a",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Key Takeaways</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03f25b-39d2-4d19-a206-99c64741cd1c",
   "metadata": {},
   "source": [
    "n_estimators:\n",
    "\n",
    "Always increase until validation score stabilizes.\n",
    "\n",
    "More trees → better stability but longer training.\n",
    "\n",
    "max_depth:\n",
    "\n",
    "Primary control for overfitting. Restrict if trees are too complex.\n",
    "\n",
    "max_features:\n",
    "\n",
    "Critical for diversity. Lower values reduce correlation between trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253cc22-8a17-41a8-9fcc-bbac8daaae94",
   "metadata": {},
   "source": [
    "<h5 style='color:black;'>Golden Rule:</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359f989-f204-4570-9777-f94071252ca5",
   "metadata": {},
   "source": [
    "Start conservative (modest depth, default features), then increase complexity only if underfitting occurs. Always validate with out-of-bag error or cross-validation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
