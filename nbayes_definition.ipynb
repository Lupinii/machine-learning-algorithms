{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de9a163-938e-4abe-b05f-db2779262857",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Naive Bayes Algorithm</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b03c7d-9f85-4e49-8323-2b3b832d63a8",
   "metadata": {},
   "source": [
    "The Naive Bayes algorithm is a probabilistic classification method based on Bayes' Theorem, relying on a key (often unrealistic) assumption of feature independence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74dffae-11ab-4e4a-aa05-1388cc7345dc",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Comparing Gaussian, Multinomial, and Bernoulli Naive Bayes classifiers.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d7a9b-650a-410d-bd06-4b615a0664ae",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>1. Gaussian Naive Bayes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b684d-d7eb-4e3f-a904-f42a4367a09b",
   "metadata": {},
   "source": [
    "Best for: Continuous, real-valued data (e.g., sensor readings, measurements).\n",
    "\n",
    "Assumptions: Features follow a normal distribution per class.\n",
    "\n",
    "Strengths:\n",
    "\n",
    "Naturally handles continuous data without discretization.\n",
    "\n",
    "Works well with naturally Gaussian data (e.g., heights, temperatures).\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "Fails if data is skewed/multimodal.\n",
    "\n",
    "Sensitive to outliers.\n",
    "\n",
    "Example: Classifying iris species based on petal/sepal measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a11e7-97b8-4f5e-8f02-f5984b93fefb",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>2. Multinomial Naive Bayes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b390264-0675-4f59-a970-fcc62fa8031c",
   "metadata": {},
   "source": [
    "Best for: Discrete count data (e.g., word frequencies, item counts).\n",
    "\n",
    "Assumptions: Features represent event frequencies (non-negative integers).\n",
    "\n",
    "Strengths:\n",
    "\n",
    "Ideal for text classification (e.g., spam detection, topic labeling).\n",
    "\n",
    "Handles TF (term frequency) well.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "Ignores feature absence (0 values).\n",
    "\n",
    "Unsuitable for continuous data.\n",
    "\n",
    "Example: Sentiment analysis using word counts in reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602a86a-109e-4939-a203-473c103ad3eb",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>3. Bernoulli Naive Bayes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dc1e52-60ea-4d34-ba62-5d4bd8100f42",
   "metadata": {},
   "source": [
    "Best for: Binary features (e.g., presence/absence, true/false).\n",
    "\n",
    "Assumptions: Features are binary variables (Bernoulli trials).\n",
    "\n",
    "Strengths:\n",
    "\n",
    "Focuses on feature presence/absence.\n",
    "\n",
    "Works well with short text or categorical data.\n",
    "\n",
    "Weaknesses:\n",
    "\n",
    "Penalizes absence (0 values), which may not always be meaningful.\n",
    "\n",
    "Cannot model frequency information.\n",
    "\n",
    "Example: Detecting diseases based on symptom presence (yes/no).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b22d6-6b3f-442f-a470-de74634f621e",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Practical Recommendations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf569d-a1f4-490d-b23c-4bb8a3a5671c",
   "metadata": {},
   "source": [
    "Text Data:\n",
    "\n",
    "Use Multinomial NB for document classification with word frequencies.\n",
    "\n",
    "Use Bernoulli NB for short documents (e.g., tweets) where presence/absence matters most.\n",
    "\n",
    "Continuous Data:\n",
    "\n",
    "Use Gaussian NB if features are approximately normally distributed.\n",
    "\n",
    "If not, consider transforming data (e.g., log-transform) or using other models.\n",
    "\n",
    "Categorical Data:\n",
    "\n",
    "Bernoulli NB for binary features.\n",
    "\n",
    "For multi-category features, use one-hot encoding with Multinomial NB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d3368-0772-4cd9-8bdc-83e739dbbd43",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>A step-by-step implementation of a Naive Bayes classifier on a real dataset using scikit-learn, including data preprocessing and evaluation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721aa72e-0dba-4ecb-bc83-0ab6846b72cc",
   "metadata": {},
   "source": [
    "1. Load the Iris dataset.\n",
    "\n",
    "2. Preprocess the data (if necessary, but Iris is clean).\n",
    "\n",
    "3. Split the data into training and testing sets.\n",
    "\n",
    "4. Choose a Naive Bayes variant (Gaussian NB is suitable for continuous features).\n",
    "\n",
    "5. Train the classifier on the training set.\n",
    "\n",
    "6. Make predictions on the testing set.\n",
    "\n",
    "7. Evaluate the model (accuracy, confusion matrix, classification report).\n",
    "\n",
    "Alternatively, for text data we might use Multinomial or Bernoulli, but Iris is continuous so we use Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa517b42-0a56-482c-9e21-f20b0cf68f6c",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>1. Import Required Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6df562-4dea-422f-b742-9205075f3eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f4b7b-5e1a-4b26-a314-ac645fd03f79",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>2. Load and Explore the Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0d9b48-9089-46b7-a184-eaf8c5f5045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (sepal/petal measurements)\n",
    "y = iris.target  # Target (species: 0=setosa, 1=versicolor, 2=virginica)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['species'] = y\n",
    "print(df.head())\n",
    "print(\"\\nClass distribution:\\n\", df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6587ebd-5033-4663-80c4-81e928b14c8b",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>3. Preprocess Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8ebb7-2bfb-41e3-9b0e-db9fc4312793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training (70%) and testing (30%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features (optional but often improves Gaussian NB)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d5096-682c-44e4-b8a2-98f81d81bd9a",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>4. Train Gaussian Naive Bayes Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998b6e5-ce47-44c4-b465-19c9624ebba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c183c-4ebc-445d-9e73-cb0be2118c2e",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>5. Evaluate the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3dc4b-f4cd-4845-9899-bb2df14f0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix, \"\\n\")\n",
    "\n",
    "# Classification report\n",
    "class_report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22673a1a-7bf0-4646-bea0-40b70868ffd8",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Output Example</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c24496-fd05-4d04-8ebe-ed0047de231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.98\n",
    "\n",
    "Confusion Matrix:\n",
    "[[15  0  0]\n",
    " [ 0 14  1]\n",
    " [ 0  0 15]] \n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "      setosa       1.00      1.00      1.00        15\n",
    "  versicolor       1.00      0.93      0.97        15\n",
    "   virginica       0.94      1.00      0.97        15\n",
    "\n",
    "    accuracy                           0.98        45\n",
    "   macro avg       0.98      0.98      0.98        45\n",
    "weighted avg       0.98      0.98      0.98        45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db21e09-cfc4-44a4-8c7d-6a747c0b715c",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Key Explanations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153c267-4d49-45c8-81f7-db91209dd7cc",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Data Splitting:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eed7e3-7485-4bf7-b0b2-053a52cc89d2",
   "metadata": {},
   "source": [
    "stratify=y ensures balanced class distribution in train/test splits.\n",
    "\n",
    "Test size = 30% (45 samples) with 70% training (105 samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a0294-d331-4bf0-9d54-16dfbe5a6f97",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Preprocessing:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc790e-0774-4895-b0a5-f2a84c0a1fa5",
   "metadata": {},
   "source": [
    "Standard scaling (mean=0, std=1) improves Gaussian NB’s performance since it assumes features are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ceeaa-6055-42f8-8e97-cf98bdd488f1",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Model Training</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833bb6ec-1877-4ad0-8fc7-386a1ac5dc3b",
   "metadata": {},
   "source": [
    "GaussianNB estimates mean (μ) and standard deviation (σ) for each feature per class.\n",
    "\n",
    "Predictions use Bayes’ theorem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d75c0-8fda-4e52-ae43-f80c4d2a86e5",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Evaluation:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4500083-57a3-4d05-8e1b-2c6ecba4e0c3",
   "metadata": {},
   "source": [
    "Accuracy: 98% here (misclassified only 1 versicolor as virginica).\n",
    "\n",
    "Confusion Matrix: Diagonals show correct predictions.\n",
    "\n",
    "Precision/Recall: High for all classes (no significant bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5aa5a-d72a-4a3e-9fed-43c65582881f",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>When to Use Other Variants?</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4b801-a2a0-453c-b9df-6cdd0a4cbf2b",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>MultinomialNB (e.g., Text Classification):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d67c3-c92f-4f16-bfef-811900291cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example: 20 Newsgroups dataset\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(text_train)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6272eae6-bce6-4a01-b384-8fa8ed8c5908",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>BernoulliNB (e.g., Binary Features):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc12351-048a-4f52-8c57-a506f17a7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# For binary features (e.g., word presence/absence)\n",
    "model = BernoulliNB(binarize=0.5)  # Threshold=0.5\n",
    "model.fit(X_train_binary, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c15a78-4540-4f7a-ad5a-cf288ea574ff",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Real-World Recommendations</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0649f51-6bd1-4bed-bfd5-221ac4af3e70",
   "metadata": {},
   "source": [
    "Iris-like Data (Continuous Features): GaussianNB\n",
    "\n",
    "Text Data (Word Counts): MultinomialNB\n",
    "\n",
    "Binary Features (e.g., Medical Symptoms): BernoulliNB\n",
    "\n",
    "Always Preprocess: Scale continuous features, use TF-IDF for text, handle missing values.\n",
    "\n",
    "Baseline Model: Naive Bayes trains in milliseconds – ideal for quick prototyping!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39061f-2a59-4db2-b913-6b50132058ec",
   "metadata": {},
   "source": [
    "<h3 style='color:black'> Strengths and weaknesses of the Naive Bayes algorithm.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441929fc-c911-498f-8334-6b5535bf7cbb",
   "metadata": {},
   "source": [
    "Strengths of Naive Bayes:\n",
    "\n",
    "1. **Efficiency and Speed**:\n",
    "\n",
    "- Training and prediction are very fast because the model only requires computing the conditional probabilities of features given the class and the class priors.\n",
    "\n",
    "- Especially in high-dimensional data (like text), NB can be trained with a single pass through the data, making it linear in the number of features and examples.\n",
    "\n",
    "2. **Performs well with high-dimensional data**:\n",
    "\n",
    "- Even when the number of features is very large (e.g., thousands of words in text classification), NB remains computationally feasible and often performs surprisingly well.\n",
    "\n",
    "- The independence assumption helps to avoid the curse of dimensionality to some extent because each feature is considered independently.\n",
    "\n",
    "3. **Decent performance with small datasets**:\n",
    "\n",
    "- Due to its simplicity and the use of maximum likelihood estimates (with smoothing), NB can perform reasonably well even when the training data is limited.\n",
    "\n",
    "4. **Handles irrelevant features relatively well**:\n",
    "\n",
    "- Because each feature is treated independently, an irrelevant feature may not affect the overall classification as much as in models that consider feature interactions.\n",
    "\n",
    "5. **Natural handling of multiclass problems**:\n",
    "\n",
    "- NB is inherently a multiclass classifier, whereas Logistic Regression in its basic form is binary and requires extensions (e.g., one-vs-rest) for multiclass.\n",
    "\n",
    "Weaknesses of Naive Bayes:\n",
    "\n",
    "1. **Feature Independence Assumption**:\n",
    "\n",
    "- The assumption that features are conditionally independent given the class is often violated in real-world data. For example, in text, words are often correlated. This can lead to overconfident probability estimates and suboptimal performance.\n",
    "\n",
    "2. **Cannot learn interactions between features**:\n",
    "\n",
    "- Because of the independence assumption, NB cannot capture interactions between features (e.g., if feature A and feature B together are a strong indicator, but individually are not). In contrast, decision trees and logistic regression (if polynomial features are included) can capture interactions.\n",
    "\n",
    "3. **Probability estimates can be unreliable**:\n",
    "\n",
    "- The predicted probabilities from NB are often not well-calibrated (especially when the independence assumption is violated). Logistic regression, on the other hand, produces well-calibrated probabilities.\n",
    "\n",
    "4. **Sensitive to imbalanced classes**:\n",
    "\n",
    "- While the prior probabilities can account for imbalance, if the imbalance is severe and the feature distributions are not estimated well in the minority class, performance may suffer.\n",
    "\n",
    "5. **Data scarcity for a feature**:\n",
    "\n",
    "- If a particular feature-category combination is not observed in the training data, the conditional probability may be zero (unless smoothing is applied). Smoothing helps, but it's an additional consideration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
