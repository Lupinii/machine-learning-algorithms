{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48fe616-9a19-4b6b-87e7-722cf8d94c28",
   "metadata": {},
   "source": [
    "<h3 style='color:green;'>Decision Trees Algorithm</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d3383-f154-401d-9d86-d8a3876bb2e0",
   "metadata": {},
   "source": [
    "\n",
    "We are going to explain the Decision Tree algorithm step by step, focusing on classification trees (though regression trees are similar in spirit).\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Start at the root node with the entire dataset.\n",
    "\n",
    "2. Select the best attribute and a split condition to partition the data into subsets (child nodes) based on a criterion (like Gini impurity or entropy for classification, variance for regression).\n",
    "\n",
    "3. Recursively repeat the process for each child node until a stopping condition is met (e.g., maximum depth, minimum samples per leaf, pure node).\n",
    "\n",
    "4. Assign a class label to the leaf node (majority class in classification, average value in regression).\n",
    "\n",
    "How splits are determined?\n",
    "\n",
    "- For each feature, we consider possible split points (for continuous features, we sort and consider midpoints between adjacent distinct values; for categorical, we consider subsets or one-hot encoding? but note: common approach is to use a greedy search over all possible splits).\n",
    "\n",
    "- We evaluate the quality of each split using a criterion (e.g., Gini index or information gain).\n",
    "\n",
    "Let's define:\n",
    "\n",
    "- Gini impurity: Gini(D) = 1 - Σ (p_i)^2, where p_i is the probability of class i in the node.\n",
    "\n",
    "- Information gain (based on entropy): IG(D, split) = Entropy(D) - Σ [ |D_j|/|D| * Entropy(D_j) ]\n",
    "\n",
    "where Entropy(D) = - Σ p_i * log2(p_i)\n",
    "\n",
    "For regression, we often use variance reduction.\n",
    "\n",
    "Step-by-step:\n",
    "\n",
    "1. Initialize:\n",
    "\n",
    "- Start with the entire dataset at the root.\n",
    "\n",
    "2. For each node:\n",
    "\n",
    "- If the stopping criteria are met (e.g., the node is pure, or the depth limit is reached, or the number of samples is below a threshold), then create a leaf node and assign it the majority class (or the mean value for regression).\n",
    "\n",
    "- Otherwise, for each feature and each possible split point of that feature, calculate the impurity (or the information gain) of the split.\n",
    "\n",
    "3. Choose the best split:\n",
    "\n",
    "- The split that maximizes the information gain (or minimizes the impurity) is chosen.\n",
    "\n",
    "4. Split the node:\n",
    "\n",
    "- Create child nodes for each subset of the data resulting from the split.\n",
    "\n",
    "5. Recurse:\n",
    "\n",
    "- Apply the same process recursively on each child node.\n",
    "\n",
    "Example for classification with Gini impurity:\n",
    "\n",
    "Suppose we have a dataset at a node with two classes: [5 positive, 5 negative].\n",
    "\n",
    "Gini impurity = 1 - ( (5/10)^2 + (5/10)^2 ) = 1 - (0.25+0.25) = 0.5.\n",
    "\n",
    "Now consider a split that creates two subsets:\n",
    "\n",
    "Subset1: [3 positive, 1 negative] -> Gini1 = 1 - ( (3/4)^2 + (1/4)^2 ) = 1 - (0.5625 + 0.0625) = 0.375.\n",
    "\n",
    "Subset2: [2 positive, 4 negative] -> Gini2 = 1 - ( (2/6)^2 + (4/6)^2 ) = 1 - (0.1111 + 0.4444) ≈ 0.4444.\n",
    "\n",
    "The weighted average impurity for the split: (4/10)*0.375 + (6/10)*0.4444 ≈ 0.15 + 0.26664 = 0.41664.\n",
    "\n",
    "The reduction in impurity (Gini gain) = 0.5 - 0.41664 = 0.08336.\n",
    "\n",
    "We would compare this gain with gains from other splits and choose the split with the highest gain.\n",
    "\n",
    "Alternatively, we can use information gain (with entropy):\n",
    "\n",
    "Entropy before split: - (0.5 * log2(0.5) + 0.5 * log2(0.5)) = 1.\n",
    "\n",
    "Entropy for Subset1: - ( (3/4)*log2(3/4) + (1/4)*log2(1/4) ) ≈ - (0.75*(-0.415) + 0.25*(-2)) ≈ - ( -0.311 -0.5 ) ≈ 0.811.\n",
    "\n",
    "Entropy for Subset2: - ( (2/6)*log2(2/6) + (4/6)*log2(4/6) ) ≈ - ( (1/3)*(-1.585) + (2/3)*(-0.585) ) ≈ - ( -0.528 -0.39 ) ≈ 0.918.\n",
    "\n",
    "Weighted average entropy: (4/10)*0.811 + (6/10)*0.918 ≈ 0.3244 + 0.5508 = 0.8752.\n",
    "\n",
    "Information gain = 1 - 0.8752 = 0.1248.\n",
    "\n",
    "6. Stopping criteria:\n",
    "\n",
    "- The node becomes pure (all samples belong to one class).\n",
    "\n",
    "- The tree reaches the maximum depth (a hyperparameter we set).\n",
    "\n",
    "- The number of samples in the node is below a minimum threshold for splitting.\n",
    "\n",
    "- The improvement in the split criterion (gain) is below a threshold.\n",
    "\n",
    "7. Pruning (optional):\n",
    "\n",
    "- After the tree is fully grown, we can prune it to avoid overfitting (by removing branches that have little impact on the accuracy, using a validation set).\n",
    "\n",
    "For regression:\n",
    "\n",
    "- Instead of Gini or entropy, we use the variance of the target values in the node.\n",
    "\n",
    "- The quality of a split is measured by the reduction in variance (or mean squared error).\n",
    "\n",
    "- The leaf node predicts the average of the target values in that leaf.\n",
    "\n",
    "Important:\n",
    "\n",
    "- Decision trees are prone to overfitting, so we use hyperparameters (max_depth, min_samples_split, min_samples_leaf, etc.) to control the growth.\n",
    "\n",
    "Let's write the step-by-step algorithm in a structured way:\n",
    "\n",
    "Algorithm: Decision Tree Learning (Classification)\n",
    "\n",
    "function BuildTree(node, data, depth):\n",
    "\n",
    "1. If stopping_condition(data) is True:\n",
    "\n",
    "node.label = majority_class(data)\n",
    "\n",
    "return\n",
    "\n",
    "2. best_split = None\n",
    "\n",
    "best_gain = -infinity\n",
    "\n",
    "3. For each feature in data.features:\n",
    "\n",
    "For each possible split point in that feature (for continuous, sort and take midpoints; for categorical, consider each subset? but note: for categorical we often use one-vs-all or multi-way splits? but in practice, for a categorical feature with k categories, we consider 2^(k-1)-1 splits, which is expensive so we often use a greedy approach or treat as ordinal? Alternatively, for binary splits, we can consider grouping categories by the proportion of the target variable. However, common libraries like scikit-learn only support binary splits for all features and use an efficient algorithm for continuous features).\n",
    "\n",
    "Actually, for a continuous feature:\n",
    "\n",
    "- Sort the data by that feature.\n",
    "\n",
    "- Consider each midpoint between adjacent values that have different target labels.\n",
    "\n",
    "For a categorical feature, we can try to split by each category (if binary) or by grouping categories? But note: scikit-learn uses an efficient method by sorting the categories by the mean of the target (for regression) or by the class probability (for classification) and then treating it as ordinal? Then the splits are of the form: category in set S vs not in S? But the set S is contiguous in the sorted order? So we only need k-1 splits for k categories.\n",
    "\n",
    "4. For each candidate split:\n",
    "\n",
    "left_data, right_data = split_data(data, candidate_split)\n",
    "\n",
    "current_gain = information_gain(data, left_data, right_data)   # or Gini gain\n",
    "\n",
    "If current_gain > best_gain:\n",
    "\n",
    "best_gain = current_gain\n",
    "\n",
    "best_split = candidate_split\n",
    "\n",
    "5. If best_gain is above a threshold (or if we found a split that improves):\n",
    "\n",
    "node.split = best_split\n",
    "\n",
    "left_child = new Node()\n",
    "\n",
    "right_child = new Node()\n",
    "\n",
    "BuildTree(left_child, data_left, depth+1)\n",
    "\n",
    "BuildTree(right_child, data_right, depth+1)\n",
    "\n",
    "node.children = [left_child, right_child]\n",
    "\n",
    "6. Else:   # no split found that improves enough\n",
    "\n",
    "node.label = majority_class(data)\n",
    "\n",
    "How to compute information gain (using entropy) for a split?\n",
    "\n",
    "IG(D, split) = Entropy(D) - [ (|D_left|/|D|)*Entropy(D_left) + (|D_right|/|D|)*Entropy(D_right) ]\n",
    "\n",
    "Similarly for Gini gain:\n",
    "\n",
    "Gini_gain = Gini(D) - [ (|D_left|/|D|)*Gini(D_left) + (|D_right|/|D|)*Gini(D_right) ]\n",
    "\n",
    "Note: There are other criteria, e.g., CART (Classification and Regression Trees) uses Gini for classification and variance for regression.\n",
    "\n",
    "Also note: For multi-way splits (more than two children), the formula extends naturally.\n",
    "\n",
    "However, in practice, binary splits are common because multi-way splits can fragment the data too quickly.\n",
    "\n",
    "Additional considerations:\n",
    "\n",
    "- Missing values: Some algorithms (like C4.5) handle missing values by using surrogate splits or by distributing the sample to all children with weights. In scikit-learn, missing values are not supported by default.\n",
    "\n",
    "- Efficiency: For continuous features, we can compute the gain for each split point in O(n log n) per feature (if we sort once). But note: we can compute the gain in linear time after sorting by using cumulative statistics.\n",
    "\n",
    "- Cost complexity pruning: After building the tree, we can prune it to avoid overfitting.\n",
    "\n",
    "Let's summarize the steps in a clear, step-by-step manner:\n",
    "\n",
    "Step 1: Start at the root node with the entire dataset.\n",
    "\n",
    "Step 2: Check stopping conditions for the current node:\n",
    "\n",
    "- If all samples in the node belong to the same class, then make it a leaf node and assign that class.\n",
    "\n",
    "- If the node depth is equal to the maximum depth (if set), then make it a leaf and assign the majority class.\n",
    "\n",
    "- If the number of samples is less than the minimum required to split, then make it a leaf and assign the majority class.\n",
    "\n",
    "Step 3: For each feature, find the best split point (if the feature is continuous, we consider midpoints; if categorical, we consider subsets as described).\n",
    "\n",
    "Step 4: Among all the features and their best split points, choose the split that gives the highest information gain (or Gini gain).\n",
    "\n",
    "Step 5: If the highest gain is less than a minimum gain threshold (if set), then stop and make it a leaf.\n",
    "\n",
    "Step 6: Split the current node into child nodes according to the chosen split.\n",
    "\n",
    "Step 7: Recursively apply steps 2-6 for each child node.\n",
    "\n",
    "Step 8: Return the tree.\n",
    "\n",
    "This is a high-level explanation. Real-world implementations (like in scikit-learn) use optimized versions of this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dcd935-158c-4cb6-bc0e-19c0ad7e8e50",
   "metadata": {},
   "source": [
    "<h5>Step-by-Step Explanation of the Decision Tree Algorithm</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c57272d-2183-4946-a351-5a92962f1d66",
   "metadata": {},
   "source": [
    "Core Idea:\n",
    "Decision trees partition the feature space into regions (nodes) using hierarchical splits, aiming to maximize homogeneity (purity) in the target variable within each region. The tree grows by recursively splitting nodes until stopping criteria are met.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b13425d-d62f-4989-b4f5-aaab0e9d329d",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Step 1: Start at the Root Node</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d06c74-7a37-4caa-adac-1e6d52c9412d",
   "metadata": {},
   "source": [
    "Begin with the entire dataset at the root node.\n",
    "\n",
    "Calculate the impurity of the node (e.g., Gini index or entropy for classification; variance for regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e96843f-a800-4c0f-a5e4-186036470263",
   "metadata": {},
   "source": [
    "Key Metrics:\n",
    "\n",
    "Gini Index (Classification):\n",
    "Lower Gini = higher purity.\n",
    "\n",
    "Entropy (Classification):\n",
    "Lower entropy = higher purity.\n",
    "\n",
    "Variance (Regression):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff19aaf-3251-475c-8d8b-749f4f8f3480",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Step 2: Find the Best Split</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3707c105-aa87-4823-982a-a17ba7b46cfe",
   "metadata": {},
   "source": [
    "For every feature and every possible split point:\n",
    "\n",
    "(i)Split the Data:\n",
    "For numerical features: Test midpoints between sorted values.\n",
    "For categorical features;\n",
    "\n",
    "(ii)Calculate Impurity Reduction:\n",
    "Information Gain (IG) = Parent impurity – Weighted child impurities:\n",
    "Choose the split with highest IG (or lowest weighted child impurity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c38f98-d230-487a-9018-f86380f32fd0",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Step 3: Split the Node\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad355d-aa85-4ec3-adba-6d6dba1b2af8",
   "metadata": {},
   "source": [
    "Partition the data into left/right child nodes using the best split.\n",
    "\n",
    "Example:\n",
    "Split: Age≤30 → Left node (samples ≤ 30), Right node (samples > 30).\n",
    "\n",
    "Purity Check: If left node has mostly class \"Yes\", it becomes purer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4693f1-7af5-4bc5-9cb2-11398bfc34ef",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Step 4: Recursively Repeat</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c768ad-4d4a-4e87-b1d3-1efde903692f",
   "metadata": {},
   "source": [
    "Repeat Steps 1–3 for each child node.\n",
    "Continue until a stopping criterion is met:\n",
    "\n",
    "Node reaches maximum depth (hyperparameter).\n",
    "\n",
    "Node contains ≤ min_samples_split (hyperparameter).\n",
    "\n",
    "Impurity reduction ≤ threshold (e.g., IG < 0.001).\n",
    "\n",
    "All samples in a node belong to one class (perfect purity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e321a0d-bd3c-490c-9ba8-7aeb932b0172",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Step 5: Assign Leaf Node Labels</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb142b8-2c14-4577-a4c3-5719981f47cf",
   "metadata": {},
   "source": [
    "When splitting stops, convert terminal nodes into leaf nodes:\n",
    "\n",
    "Classification: Predict the majority class.\n",
    "\n",
    "Regression: Predict the mean target value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f347e4-00f6-445c-9040-375066e26392",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Key Notes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e955ff-a986-4395-a312-9ae93252a42c",
   "metadata": {},
   "source": [
    "Greedy Algorithm: Selects locally optimal splits at each step (no backtracking).\n",
    "\n",
    "Hyperparameters: Control overfitting via max_depth, min_samples_split, min_impurity_decrease.\n",
    "\n",
    "Advantages: Interpretable, handles nonlinear relationships.\n",
    "\n",
    "Limitations: Prone to overfitting; sensitive to small data changes (use ensembles like Random Forests).\n",
    "\n",
    "By following these steps, decision trees iteratively partition data into purer subsets, creating a hierarchical structure for prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8e329-a591-40d3-92b0-65c571e9e053",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>A complete example of training a Decision Tree on a real dataset using Python, including preprocessing and evaluation.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d8af7-d510-4edb-bf0f-5bdcf1a14a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load and Explore Data\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='species')\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Feature names:\", iris.feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "print(\"\\nFirst 5 samples:\\n\", X.head())\n",
    "print(\"\\nClass distribution:\\n\", y.value_counts())\n",
    "\n",
    "# Step 3: Preprocessing\n",
    "# No missing values in Iris dataset - check with:\n",
    "print(\"\\nMissing values:\\n\", X.isnull().sum())\n",
    "\n",
    "# Split into train/test sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 4: Train Decision Tree\n",
    "# Using Gini impurity with max_depth=3 to prevent overfitting\n",
    "model = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate Model\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "print(\"\\nTest Accuracy:\", f\"{accuracy:.2f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "# Step 6: Visualize the Tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(\n",
    "    model,\n",
    "    feature_names=iris.feature_names,\n",
    "    class_names=target_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    proportion=True\n",
    ")\n",
    "plt.title(\"Decision Tree for Iris Classification\")\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Feature Importance Analysis\n",
    "feature_importances = pd.Series(\n",
    "    model.feature_importances_,\n",
    "    index=iris.feature_names\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\\n\", feature_importances)\n",
    "feature_importances.plot(kind='bar', title='Feature Importance')\n",
    "plt.ylabel('Importance Score (0-1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b6ea0-18ee-4d1d-b535-6078417749c4",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Dataset Loading</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5c5fe-923c-4732-a4d4-e928abab2a48",
   "metadata": {},
   "source": [
    "Uses the built-in Iris dataset (150 samples, 4 features)\n",
    "\n",
    "3 flower species to classify (setosa, versicolor, virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71c7f0-4aa8-4a0d-974a-305391279f93",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8fae6-fc01-4eab-8fea-56fc439a5759",
   "metadata": {},
   "source": [
    "Checks for missing values (none in Iris)\n",
    "\n",
    "Splits data into 80% training (120 samples) and 20% testing (30 samples)\n",
    "\n",
    "Uses stratified sampling to maintain class balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab4f29-27a8-45d4-8d10-4eac70ce2cb1",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Model Training</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cc3b9-e32c-4c15-9adc-82d24756a491",
   "metadata": {},
   "source": [
    "Uses Gini impurity as the splitting criterion\n",
    "\n",
    "Limits tree depth to 3 (max_depth=3) to prevent overfitting\n",
    "\n",
    "Other hyperparameters: Default settings for min_samples_split (2) and min_samples_leaf (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912cc20-8f32-4e8a-ba64-90d05c704d04",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e6232-1ff9-414b-a25e-e27f6ea72a65",
   "metadata": {},
   "source": [
    "Accuracy: Percentage of correct predictions\n",
    "\n",
    "Confusion Matrix: Shows true vs. predicted classes\n",
    "\n",
    "Classification Report: Precision, recall, and F1-score per class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfc7743-0525-4d5d-b483-139135e01ef4",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Visualization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8625fc7e-f547-4d95-be19-257f9a961afa",
   "metadata": {},
   "source": [
    "Plots the decision tree with class colors and split thresholds\n",
    "\n",
    "Shows feature importances (petal dimensions usually dominate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50787e9-c493-4b16-8870-b79ff1a6caee",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Sample Output</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab025d74-4418-4e5d-a247-80692e6c3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test Accuracy: 0.97\n",
    "\n",
    "Confusion Matrix:\n",
    " [[10  0  0]\n",
    " [ 0  9  1]\n",
    " [ 0  0 10]]\n",
    "\n",
    "Classification Report:\n",
    "              precision  recall  f1-score   support\n",
    "      setosa       1.00      1.00      1.00        10\n",
    "  versicolor       1.00      0.90      0.95        10\n",
    "   virginica       0.91      1.00      0.95        10\n",
    "\n",
    "    accuracy                           0.97        30\n",
    "   macro avg       0.97      0.97      0.97        30\n",
    "weighted avg       0.97      0.97      0.97        30\n",
    "\n",
    "Feature Importances:\n",
    " petal length (cm)    0.666\n",
    "petal width (cm)     0.334\n",
    "sepal length (cm)    0.000\n",
    "sepal width (cm)     0.000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5673d-24d7-496e-bfb7-1ea8d35568a3",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Interpretation:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ed71d-ecf1-49c4-906c-b4ef673e06e8",
   "metadata": {},
   "source": [
    "High Accuracy: 97% on test data (misclassified only 1 versicolor as virginica)\n",
    "\n",
    "Key Splits:\n",
    "\n",
    "First split: Petal length ≤ 2.45 cm → immediately identifies setosa\n",
    "\n",
    "Subsequent splits use petal width/length to separate versicolor/virginica\n",
    "\n",
    "Feature Importance:\n",
    "\n",
    "Petal dimensions account for 100% of the splitting decisions\n",
    "\n",
    "Sepal measurements are irrelevant for this tre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354946d4-4a1a-492f-ab84-67bf3c7fe538",
   "metadata": {},
   "source": [
    "<h3 style='color:white;'>Best Practises</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d973cc-e56d-4231-ab38-c1bd34cf06e7",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350704d1-9aca-4ae2-9387-215e3a9b3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(), params, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb098fc-c02a-4a68-9651-1d907f5c31f1",
   "metadata": {},
   "source": [
    "Handling Overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf26847-a784-4998-b31a-d55488f8e7ce",
   "metadata": {},
   "source": [
    "If accuracy on training is much higher than testing:\n",
    "\n",
    "Increase min_samples_split or min_samples_leaf\n",
    "\n",
    "Reduce max_depth\n",
    "\n",
    "Use pruning (ccp_alpha parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c2490-d44d-485d-9d99-7f1b5f54192f",
   "metadata": {},
   "source": [
    "For Larger Datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c28825-032e-4ee5-ad89-287a54e9bb51",
   "metadata": {},
   "source": [
    "Use class_weight='balanced' for imbalanced classes\n",
    "\n",
    "Consider binning continuous features\n",
    "\n",
    "Use max_features to limit features per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246fd2ae-fe0d-4b33-804a-829a912bf406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
