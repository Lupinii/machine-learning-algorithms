{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf1c770-e815-41c7-a026-c7e80e149dec",
   "metadata": {},
   "source": [
    "<h3 style='color:green'>Principal Component Analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1cd58c-4e7f-434a-82e8-7a9a20c7992e",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique widely used in machine learning to simplify data by reducing the number of variables while retaining most of the original information.\n",
    "\n",
    "It achieves this by identifying principal components, which are new variables that are linear combinations of the original features, and then selecting the most important ones.\n",
    "\n",
    "These components are ranked by the amount of variance they explain, allowing for the discarding of less important components and reducing the dataset's dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fad0f-6b0a-435d-8d00-936cbf6ba83d",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>What PCA does:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64040a4-db07-4be0-bcfb-64d7b23a72f6",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Reduces dimensionality:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45942e19-b4e4-46fb-b728-ad5bbc3d1ef1",
   "metadata": {},
   "source": [
    "Dimensionlity reduction is a technique that reduces the number of input variables in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13b1ae-772c-4052-b734-a29d471b20bb",
   "metadata": {},
   "source": [
    "PCA transforms high-dimensional data into a lower-dimensional space, making it easier to analyze and visualize, especially with datasets that have many features (the \"curse of dimensionality\"). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e49023-73ba-4299-89ca-f088ebd0311e",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Identifies principal components:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27c7228-97a5-4421-8cd4-217817941ad2",
   "metadata": {},
   "source": [
    "PCA finds new variables (principal components) that are orthogonal (uncorrelated) to each other and capture the maximum variance in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550ca7ae-41be-4a1c-95c2-9d7138d30f1f",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Retains variance:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e7c03-e8a9-4cd2-a3e2-c29ec2651a3f",
   "metadata": {},
   "source": [
    "The first few principal components capture the most significant variations in the data, allowing for the removal of less important components without significant information loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c0841b-8b70-4f90-af97-f3558283f804",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Important terminologies</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112aaf57-ad8f-4675-9b39-6f8396527f92",
   "metadata": {},
   "source": [
    "Views - It is the perspectives through which datapoints are observed.\n",
    "\n",
    "Dimension - Number of columns in a dataset.\n",
    "\n",
    "Principal Component - New variables that are constructed as linear combinations or mixture of the initail variables.\n",
    "\n",
    "Projection - The perpendicular distance between the principal component and the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9ec3fb-732c-49d2-868a-ebcc73e9217b",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>How PCA works:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594a5034-9acc-488c-88c8-11bd840d7dfe",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>1. Standardize the data:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a744357-7160-4427-90b7-3e5232848663",
   "metadata": {},
   "source": [
    "PCA works best with standardized data, where each feature has a mean of 0 and a standard deviation of 1. This ensures that all features contribute equally to the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968f09d-b919-4aef-8160-135513d47bce",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>2. Calculate the covariance matrix:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71102966-52ce-49d0-8170-956ea8110639",
   "metadata": {},
   "source": [
    "This matrix represents the relationships between all pairs of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c418a-c1de-48bc-9cb1-07a0ab722ab0",
   "metadata": {},
   "source": [
    "Positive covariance indicate that the value of one variable is directly proportional to other variable.\n",
    "\n",
    "Negative covariance indicate that the value of one variable is inversely propotional to other variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab816c7-e005-4e5e-a243-0555230e3de1",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>3. Compute eigenvectors and eigenvalues:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48ae32-6de4-42cb-8930-a72b39fde93b",
   "metadata": {},
   "source": [
    "Eigenvectors represent the principal components, and eigenvalues indicate the amount of variance explained by each component. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d1c3ad-184f-42ed-bc6d-2f256be5934c",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>4. Select principal components:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88665fe-ad32-48f2-b615-b862df9bbda1",
   "metadata": {},
   "source": [
    "Choose the eigenvectors (principal components) with the largest corresponding eigenvalues, as these capture the most variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404cad5-49fd-464f-886d-82275729c9a2",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>5. Transform the data:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed29afb1-86b1-4db6-9f00-402b22ce371b",
   "metadata": {},
   "source": [
    "Project the original data onto the selected principal components to get the reduced-dimensional representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd2730-4b73-4aa1-b94b-76e65df9466b",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Why PCA is important in machine learning:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562fc86-819b-4c0d-a0ad-f2efdef6f4e6",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Reduces computational cost:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a375a71-dd1d-469f-8252-74ea7e866bb4",
   "metadata": {},
   "source": [
    "By reducing dimensionality, PCA can significantly decrease the training time of machine learning models, especially those dealing with large datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f1371-07fc-42ed-8aa3-10debfecdee1",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Improves model performance:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa031e3-1c21-4c05-b218-c65c1378f428",
   "metadata": {},
   "source": [
    "PCA can help reduce overfitting, especially in high-dimensional datasets, by removing noise and irrelevant features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f9e82-379f-4282-80ee-f29a415df0c9",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Enables data visualization:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cf3acc-4b13-4aea-b85b-73a9253cc606",
   "metadata": {},
   "source": [
    "PCA allows for the projection of high-dimensional data into 2D or 3D space, making it easier to visualize and understand the data's structure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa801afa-97d7-40f0-bda0-ca7ef49e5278",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Unsupervised learning:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5d2bd-67e2-458b-b7d0-ca077800b9bb",
   "metadata": {},
   "source": [
    "PCA is an unsupervised learning algorithm, meaning it doesn't require labeled data to perform dimensionality reduction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea84cad-5b75-421e-a43e-be5a48b99a31",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Applications of PCA in machine learning:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdf542-551a-40bf-960d-52a627362136",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Image processing:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dd5806-ff52-4a39-bf81-f092fe7d33b7",
   "metadata": {},
   "source": [
    "Reducing the dimensionality of image data for tasks like image recognition and compression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d7d42-07a9-4192-9d3c-0ecf88832d4d",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Bioinformatics:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ca4c7-39de-4471-8c78-669b77ac7b4a",
   "metadata": {},
   "source": [
    "Analyzing gene expression data and other high-dimensional biological data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108a90a-8ce8-4f07-a1c2-70498b40d899",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Finance:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a187636-4509-4fe9-8694-1789e0e2fddc",
   "metadata": {},
   "source": [
    "Reducing the number of financial indicators used in risk assessment and portfolio management. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d642d6-4582-48ef-9097-9195fc6520ee",
   "metadata": {},
   "source": [
    "<h3 style='color:black'>Recommendation systems:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c061cf-c569-4fea-a143-244372e093c3",
   "metadata": {},
   "source": [
    "Feature extraction and data simplification for improving recommendation accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab6861a-c6ff-483c-8475-f8e3315ed6b2",
   "metadata": {},
   "source": [
    "In essence, PCA is a powerful tool for simplifying complex datasets while preserving the most important information, making it a valuable technique in various machine learning applications. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
